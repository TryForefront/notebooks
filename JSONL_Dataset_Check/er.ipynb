{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JSONL Dataset Checker",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### JSONL Dataset Checker\n",
        "Upload your JSONL formatted dataset as \"input.txt\" and execute the following two code blocks to:\n",
        "1. See how many training examples are in your dataset.\n",
        "2. Filter and find prompt-completion pairs that are greater than 2048 tokens. Read more: https://docs.forefront.ai/forefront/master/key-concepts#tokens\n",
        "3. Filter and find prompt-completion pairs that aren't formatted correctly.\n",
        "4. Filter and find completions that don't start with a whitespace character (\" \"). Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data\n",
        "5. Filter and find prompts that don't end in a common separator. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data\n",
        "6. Filter and find completions that don't end with \"<|endoftext|>\".\n",
        "7. Split dataset into training and validation sets to use the validation examples as test prompts.\n"
      ],
      "metadata": {
        "id": "r5lIoDw1xkin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "KM0flMcGqXDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw-6qncfqNt5"
      },
      "outputs": [],
      "source": [
        "from transformers.utils.dummy_pt_objects import FlaubertWithLMHeadModel\n",
        "import json\n",
        "from transformers import GPT2TokenizerFast\n",
        "from collections import Counter\n",
        "\n",
        "\"\"\"\n",
        "Upload your JSONL formatted dataset as input.txt and execute this code to:\n",
        "1. See how many training examples are in your dataset.\n",
        "2. Filter prompt-completion pairs that are greater than 2048 tokens.\n",
        "3. Filter completions that don't start with a whitespace character (\" \").\n",
        "4. Filter prompts that don't end in a common separator.\n",
        "5. Split your dataset into training and validation sets to use the validation examples as test prompts.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "i = 0\n",
        "num_too_long = 0\n",
        "num_bad_examples = 0\n",
        "num_no_whitespace = 0\n",
        "num_no_separator = 0\n",
        "num_no_end = 0\n",
        "arr_too_long = []\n",
        "arr_bad_examples = []\n",
        "arr_no_whitespace = []\n",
        "arr_no_separator = []\n",
        "arr_no_end = []\n",
        "separators = []\n",
        "used_separators = {}\n",
        "separator = \"\"\n",
        "\n",
        "# Number of test examples to be removed from the training set\n",
        "num_test_prompts = 5\n",
        "\n",
        "# Find the most commonly used separator\n",
        "with open('input.txt') as fin:\n",
        "  for line in fin:\n",
        "    try:\n",
        "      data = json.loads(line)\n",
        "    except:\n",
        "      continue\n",
        "      \n",
        "    prompt = data['prompt']\n",
        "    separator = prompt.rsplit('\\n', 1)[1]\n",
        "    separators.append(separator)\n",
        "\n",
        "used_separators = (Counter(separators))\n",
        "max_value = max(used_separators.values())\n",
        "# Check that a separator is used at the end of prompts more than 90% of the time\n",
        "if max_value < (i*0.9):\n",
        "  print(f'Error: No common separator is used at the end of prompts. Separators: {used_separators}')\n",
        "max_key = [k for k, v in used_separators.items() if v == max_value]\n",
        "common_separator = max_key[0]\n",
        "\n",
        "with open('input.txt') as fin:\n",
        "  with open('train.txt', 'w') as ftrain:\n",
        "    with open('test_prompts.txt', 'w') as ftest_prompts:\n",
        "      for line in fin:\n",
        "        i += 1\n",
        "        try:\n",
        "          data = json.loads(line)\n",
        "        except:\n",
        "          num_bad_examples += 1\n",
        "          arr_bad_examples.append(i)\n",
        "          continue\n",
        "          \n",
        "        prompt = data['prompt']\n",
        "        completion = data['completion']\n",
        "        full = f'{prompt}{completion}'\n",
        "\n",
        "        # Filter prompts that don't end with the common separator\n",
        "        separator = prompt.rsplit('\\n', 1)[1]\n",
        "        if separator != common_separator:\n",
        "          num_no_separator += 1\n",
        "          arr_no_separator.append(i)\n",
        "          continue\n",
        "\n",
        "        # Filter completions that don't end with <|endoftext|>\n",
        "        if not completion.endswith('<|endoftext|>'):\n",
        "          num_no_end += 1\n",
        "          arr_no_end.append(i)\n",
        "          continue\n",
        "\n",
        "        # Filter prompt-completion pairs to use for test prompts\n",
        "        if (i <= num_test_prompts):\n",
        "          ftest_prompts.write(json.dumps({'prompt': prompt, 'completion': completion}))\n",
        "          ftest_prompts.write('\\n')\n",
        "          continue\n",
        "\n",
        "        # Filter completions that don't start with a whitespace\n",
        "        if not completion.startswith(\" \"):\n",
        "          num_no_whitespace += 1\n",
        "          arr_no_whitespace.append(i)\n",
        "          continue\n",
        "\n",
        "        # Filter prompt-completion pairs that are too long (2048 tokens or greater)\n",
        "        length = len(tokenizer.encode(full))\n",
        "        if length > 2047:\n",
        "          num_too_long += 1\n",
        "          arr_too_long.append(i)\n",
        "          continue\n",
        "\n",
        "        ftrain.write(json.dumps({'prompt': prompt, 'completion': completion}))\n",
        "        ftrain.write('\\n')\n",
        "\n",
        "filtered_examples = num_bad_examples + num_too_long + num_no_whitespace + num_no_separator + num_no_end\n",
        "total_examples = i - filtered_examples\n",
        "train_examples = total_examples - num_test_prompts\n",
        "\n",
        "if i > 0:\n",
        "  print(f'\\n\\nYour dataset contains {i} prompt-completion pairs.\\n\\n###\\n\\n')\n",
        "else:\n",
        "  print('Your dataset has no prompt-completion pairs.\\n\\n###\\n\\n')\n",
        "\n",
        "if num_bad_examples > 0:\n",
        "  print(f'Error: {num_bad_examples} prompt-completion pairs are formatted incorrectly. These are rows: {arr_bad_examples}\\n\\n')\n",
        "else:\n",
        "  print('All prompt-completion pairs are formatted properly.\\n\\n')\n",
        "\n",
        "if num_too_long > 0:\n",
        "  print(f'Error: {num_too_long} prompt-completion pairs exceed 2048 tokens. These are rows: {arr_too_long}. Read more: https://docs.forefront.ai/forefront/master/key-concepts#tokens \\n\\n')\n",
        "else:\n",
        "  print('There are no prompt-completions pairs that exceed 2048 tokens. Read more: https://docs.forefront.ai/forefront/master/key-concepts#tokens \\n\\n')\n",
        "\n",
        "if num_no_whitespace > 0:\n",
        "  print(f'Error: {num_no_whitespace} completions don\\'t start with a whitespace character (\" \"). These are rows: {arr_no_whitespace}. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n')\n",
        "else:\n",
        "  print('All completions start with a whitespace character. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n')\n",
        "\n",
        "if num_no_separator > 0:\n",
        "  print(f'Error: {num_no_separator} prompt don\\'t end with the common separator. These are rows: {arr_no_separator}. Common separator: \"{common_separator}\". Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n')\n",
        "else:\n",
        "  print(f'All prompts end with a common separator. Common separator: \"{common_separator}\".\\n\\n')\n",
        "\n",
        "if num_no_end > 0:\n",
        "  print(f'Error: {num_no_separator} completions don\\'t end with <\\|endoftext\\|>. These are rows: {arr_no_separator}. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n###\\n\\n')\n",
        "else:\n",
        "  print('All completions end with \"<|endoftext|>\".\\n\\n\\n###\\n\\n')\n",
        "\n",
        "if num_bad_examples > 0:\n",
        "  print(f'{filtered_examples} prompt-completion pairs have been removed due to errors. {total_examples} prompt-completion pairs are being exported:\\n\\n\\ntrain.txt: {train_examples} training examples\\n\\n\\ntest_prompts.txt: {num_test_prompts} test examples\\n\\n\\nDownload train.txt and test_prompts.txt to start fine-tuning: https://docs.forefront.ai/forefront/guides/fine-tuning#train-a-new-fine-tuned-model \\n\\n\\nIf you don\\'t see the files after completion, refresh the page.')\n",
        "else:\n",
        "  print(f'{total_examples} prompt-completion pairs are being exported:\\n\\n\\ntrain.txt: {train_examples} training examples\\n\\n\\ntest_prompts.txt: {num_test_prompts} test examples\\n\\n\\nDownload train.txt and test_prompts.txt to start fine-tuning: https://docs.forefront.ai/forefront/guides/fine-tuning#train-a-new-fine-tuned-model \\n\\n\\nIf you don\\'t see the files after completion, refresh the page.')"
      ]
    }
  ]
}
