{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JSONL Dataset Checker",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### JSONL Dataset Checker\n",
        "Upload your JSONL formatted dataset as \"input.txt\" and execute the following two code blocks to:\n",
        "1. See how many training examples are in your dataset.\n",
        "2. Filter and find prompt-completion pairs that are greater than 2048 tokens. Read more: https://docs.forefront.ai/forefront/master/key-concepts#tokens\n",
        "3. Filter and find prompt-completion pairs that aren't formatted correctly.\n",
        "4. Filter and find completions that don't start with a whitespace character (\" \"). Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data\n",
        "5. Filter and find prompts that don't end in a common separator. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data\n",
        "6. Filter and find completions that don't end with \"<|endoftext|>\".\n",
        "7. Split dataset into training and validation sets to use the validation examples as test prompts.\n"
      ],
      "metadata": {
        "id": "r5lIoDw1xkin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "KM0flMcGqXDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bd2e53-2f2b-494f-c726-dc855acf43f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw-6qncfqNt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee39d995-b17b-4858-e357-eb5391284d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Your dataset contains 9295 prompt-completion pairs.\n",
            "\n",
            "###\n",
            "\n",
            "\n",
            "All prompt-completion pairs are formatted properly.\n",
            "\n",
            "\n",
            "Error: 3 prompt-completion pairs exceed 2048 tokens. These are rows: [1079, 1488, 7732]. Read more: https://docs.forefront.ai/forefront/master/key-concepts#tokens \n",
            "\n",
            "\n",
            "All completions start with a whitespace character. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \n",
            "\n",
            "\n",
            "All prompts end with a common separator. Common separator: \"Review:\".\n",
            "\n",
            "\n",
            "All completions end with \"<|endoftext|>\".\n",
            "\n",
            "\n",
            "###\n",
            "\n",
            "\n",
            "9292 prompt-completion pairs are being exported:\n",
            "\n",
            "\n",
            "train.txt: 9287 training examples\n",
            "\n",
            "\n",
            "test_prompts.txt: 5 test examples\n",
            "\n",
            "\n",
            "Download train.txt and test_prompts.txt to start fine-tuning: https://docs.forefront.ai/forefront/guides/fine-tuning#train-a-new-fine-tuned-model \n",
            "\n",
            "\n",
            "If you don't see the files after completion, refresh the page.\n"
          ]
        }
      ],
      "source": [
        "from transformers.utils.dummy_pt_objects import FlaubertWithLMHeadModel\n",
        "import json\n",
        "from transformers import GPT2TokenizerFast\n",
        "from collections import Counter\n",
        "\n",
        "\"\"\"\n",
        "Upload your JSONL formatted dataset as input.txt and execute this code to:\n",
        "1. See how many training examples are in your dataset.\n",
        "2. Filter prompt-completion pairs that are greater than 2048 tokens.\n",
        "3. Filter completions that don't start with a whitespace character (\" \").\n",
        "4. Filter prompts that don't end in a common separator.\n",
        "5. Split your dataset into training and validation sets to use the validation examples as test prompts.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "i = 0\n",
        "num_too_long = 0\n",
        "num_bad_examples = 0\n",
        "num_no_whitespace = 0\n",
        "num_no_separator = 0\n",
        "num_no_end = 0\n",
        "arr_too_long = []\n",
        "arr_bad_examples = []\n",
        "arr_no_whitespace = []\n",
        "arr_no_separator = []\n",
        "arr_no_end = []\n",
        "separators = []\n",
        "used_separators = {}\n",
        "separator = \"\"\n",
        "\n",
        "# Number of test examples to be removed from the training set\n",
        "num_test_prompts = 5\n",
        "\n",
        "# Find the most commonly used separator\n",
        "with open('input.txt') as fin:\n",
        "  for line in fin:\n",
        "    try:\n",
        "      data = json.loads(line)\n",
        "    except:\n",
        "      continue\n",
        "      \n",
        "    prompt = data['prompt']\n",
        "    separator = prompt.rsplit('\\n', 1)[1]\n",
        "    separators.append(separator)\n",
        "\n",
        "used_separators = (Counter(separators))\n",
        "max_value = max(used_separators.values())\n",
        "# Check that a separator is used at the end of prompts more than 90% of the time\n",
        "if max_value < (i*0.9):\n",
        "  print(f'Error: No common separator is used at the end of prompts. Separators: {used_separators}')\n",
        "max_key = [k for k, v in used_separators.items() if v == max_value]\n",
        "common_separator = max_key[0]\n",
        "\n",
        "with open('input.txt') as fin:\n",
        "  with open('train.txt', 'w') as ftrain:\n",
        "    with open('test_prompts.txt', 'w') as ftest_prompts:\n",
        "      for line in fin:\n",
        "        i += 1\n",
        "        try:\n",
        "          data = json.loads(line)\n",
        "        except:\n",
        "          num_bad_examples += 1\n",
        "          arr_bad_examples.append(i)\n",
        "          continue\n",
        "          \n",
        "        prompt = data['prompt']\n",
        "        completion = data['completion']\n",
        "        full = f'{prompt}{completion}'\n",
        "\n",
        "        # Filter prompts that don't end with the common separator\n",
        "        separator = prompt.rsplit('\\n', 1)[1]\n",
        "        if separator != common_separator:\n",
        "          num_no_separator += 1\n",
        "          arr_no_separator.append(i)\n",
        "          continue\n",
        "\n",
        "        # Filter completions that don't end with <|endoftext|>\n",
        "        if not completion.endswith('<|endoftext|>'):\n",
        "          num_no_end += 1\n",
        "          arr_no_end.append(i)\n",
        "          continue\n",
        "\n",
        "        # Filter prompt-completion pairs to use for test prompts\n",
        "        if (i <= num_test_prompts):\n",
        "          ftest_prompts.write(json.dumps({'prompt': prompt, 'completion': completion}))\n",
        "          ftest_prompts.write('\\n')\n",
        "          continue\n",
        "\n",
        "        # Filter completions that don't start with a whitespace\n",
        "        if not completion.startswith(\" \"):\n",
        "          num_no_whitespace += 1\n",
        "          arr_no_whitespace.append(i)\n",
        "          continue\n",
        "\n",
        "        # Filter prompt-completion pairs that are too long (2048 tokens or greater)\n",
        "        length = len(tokenizer.encode(full))\n",
        "        if length > 2047:\n",
        "          num_too_long += 1\n",
        "          arr_too_long.append(i)\n",
        "          continue\n",
        "\n",
        "        ftrain.write(json.dumps({'prompt': prompt, 'completion': completion}))\n",
        "        ftrain.write('\\n')\n",
        "\n",
        "filtered_examples = num_bad_examples + num_too_long + num_no_whitespace + num_no_separator + num_no_end\n",
        "total_examples = i - filtered_examples\n",
        "train_examples = total_examples - num_test_prompts\n",
        "\n",
        "if i > 0:\n",
        "  print(f'\\n\\nYour dataset contains {i} prompt-completion pairs.\\n\\n###\\n\\n')\n",
        "else:\n",
        "  print('Your dataset has no prompt-completion pairs.\\n\\n###\\n\\n')\n",
        "\n",
        "if num_bad_examples > 0:\n",
        "  print(f'Error: {num_bad_examples} prompt-completion pairs are formatted incorrectly. These are rows: {arr_bad_examples}\\n\\n')\n",
        "else:\n",
        "  print('All prompt-completion pairs are formatted properly.\\n\\n')\n",
        "\n",
        "if num_too_long > 0:\n",
        "  print(f'Error: {num_too_long} prompt-completion pairs exceed 2048 tokens. These are rows: {arr_too_long}. Read more: https://docs.forefront.ai/forefront/master/key-concepts#tokens \\n\\n')\n",
        "else:\n",
        "  print('There are no prompt-completions pairs that exceed 2048 tokens. Read more: https://docs.forefront.ai/forefront/master/key-concepts#tokens \\n\\n')\n",
        "\n",
        "if num_no_whitespace > 0:\n",
        "  print(f'Error: {num_no_whitespace} completions don\\'t start with a whitespace character (\" \"). These are rows: {arr_no_whitespace}. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n')\n",
        "else:\n",
        "  print('All completions start with a whitespace character. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n')\n",
        "\n",
        "if num_no_separator > 0:\n",
        "  print(f'Error: {num_no_separator} prompt don\\'t end with the common separator. These are rows: {arr_no_separator}. Common separator: \"{common_separator}\". Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n')\n",
        "else:\n",
        "  print(f'All prompts end with a common separator. Common separator: \"{common_separator}\".\\n\\n')\n",
        "\n",
        "if num_no_end > 0:\n",
        "  print(f'Error: {num_no_separator} completions don\\'t end with <\\|endoftext\\|>. These are rows: {arr_no_separator}. Read more: https://docs.forefront.ai/forefront/guides/fine-tuning#prepare-training-data \\n\\n###\\n\\n')\n",
        "else:\n",
        "  print('All completions end with \"<|endoftext|>\".\\n\\n\\n###\\n\\n')\n",
        "\n",
        "if num_bad_examples > 0:\n",
        "  print(f'{filtered_examples} prompt-completion pairs have been removed due to errors. {total_examples} prompt-completion pairs are being exported:\\n\\n\\ntrain.txt: {train_examples} training examples\\n\\n\\ntest_prompts.txt: {num_test_prompts} test examples\\n\\n\\nDownload train.txt and test_prompts.txt to start fine-tuning: https://docs.forefront.ai/forefront/guides/fine-tuning#train-a-new-fine-tuned-model \\n\\n\\nIf you don\\'t see the files after completion, refresh the page.')\n",
        "else:\n",
        "  print(f'{total_examples} prompt-completion pairs are being exported:\\n\\n\\ntrain.txt: {train_examples} training examples\\n\\n\\ntest_prompts.txt: {num_test_prompts} test examples\\n\\n\\nDownload train.txt and test_prompts.txt to start fine-tuning: https://docs.forefront.ai/forefront/guides/fine-tuning#train-a-new-fine-tuned-model \\n\\n\\nIf you don\\'t see the files after completion, refresh the page.')"
      ]
    }
  ]
}